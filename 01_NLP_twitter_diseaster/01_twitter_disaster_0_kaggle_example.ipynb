{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Tutorial\n",
    "\n",
    "NLP - or *Natural Language Processing* - is shorthand for a wide array of techniques designed to help machines learn from text. Natural Language Processing powers everything from chatbots to search engines, and is used in diverse tasks like sentiment analysis and machine translation.\n",
    "\n",
    "In this tutorial we'll look at this competition's dataset, use a simple technique to process it, build a machine learning model, and submit predictions for a score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")  # this is the submission set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick look at our data\n",
    "\n",
    "Let's look at our data... first, an example of what is NOT a disaster tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[\"target\"] == 0][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And one that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[\"target\"] == 1][\"text\"].values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vectors\n",
    "\n",
    "The theory behind the model we'll build in this notebook is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).\n",
    "\n",
    "We'll use scikit-learn's `CountVectorizer` to count the words in each tweet and turn them into data our machine learning model can process.\n",
    "\n",
    "Note: a `vector` is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors.todense())\n",
    "# example_train_vectors.toarray()\n",
    "# count_vectorizer.get_feature_names()\n",
    "count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model for using frequency vector - from the code, testing different CV strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tells us that:\n",
    "1. There are 54 unique words (or \"tokens\") in the first five tweets.\n",
    "2. The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that DO exist in the first tweet.\n",
    "\n",
    "Now let's create vectors for all of our tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model\n",
    "\n",
    "As we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.\n",
    "\n",
    "What we're assuming here is a _linear_ connection. So let's build a linear model and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "clf = linear_model.RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# in this exercise, I have tried 1) use the typical train test split methods (which has shuffle), 2) ShuffleSplits, and 3) StratifiedKFold with or without shuffle;\n",
    "# if not shuffle, the model's result is not as good compared with shuffling\n",
    "# cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "# cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model and see how well it does on the training data. For this we'll use `cross-validation` - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.\n",
    "\n",
    "The metric for this competition is F1, so let's use that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72340426, 0.72647489, 0.7322298 ])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=cv, scoring=\"f1\")\n",
    "# scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "# scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above scores aren't terrible! It looks like our assumption will score roughly 0.65 on the leaderboard. There are lots of ways to potentially improve on this (TFIDF, LSA, LSTM / RNNs, the list is long!) - give any of them a shot!\n",
    "\n",
    "In the meantime, let's do predictions on our training set and build a submission for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifier()"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = clf.predict(test_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model for using frequency vector; first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_cv = train_test_split(train_df, test_size=0.3, random_state=10)\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_cv.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# train_df.shape\n",
    "# ## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# # that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# # i.e. that the train and test vectors use the same set of tokens.\n",
    "# test_vectors = count_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>6126</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>Silvermoon or Ironforge</td>\n",
       "      <td>Fel Lord Zakuun is about to DIE ! #Hellfire #W...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>2739</td>\n",
       "      <td>crushed</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Nick Williams just hit another bomb. Just crus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>8665</td>\n",
       "      <td>sinkhole</td>\n",
       "      <td>Êwagger!ÌominicanÌ÷</td>\n",
       "      <td>#LoMasVisto THOUSANDS OF HIPSTERS FEARED LOST:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>3184</td>\n",
       "      <td>deluge</td>\n",
       "      <td>617-BTOWN-BEATDOWN</td>\n",
       "      <td>Photo: boyhaus: Heaven sent by JakeåÊ ÛÏAfter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>7481</td>\n",
       "      <td>obliteration</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Which is true to an extent. The obliteration o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       keyword                 location  \\\n",
       "2279  6126      hellfire  Silvermoon or Ironforge   \n",
       "2280  2739       crushed             Pennsylvania   \n",
       "2281  8665      sinkhole    Êwagger!ÌominicanÌ÷   \n",
       "2282  3184        deluge       617-BTOWN-BEATDOWN   \n",
       "2283  7481  obliteration                 Illinois   \n",
       "\n",
       "                                                   text  target  \n",
       "2279  Fel Lord Zakuun is about to DIE ! #Hellfire #W...       1  \n",
       "2280  Nick Williams just hit another bomb. Just crus...       0  \n",
       "2281  #LoMasVisto THOUSANDS OF HIPSTERS FEARED LOST:...       1  \n",
       "2282  Photo: boyhaus: Heaven sent by JakeåÊ ÛÏAfter...       0  \n",
       "2283  Which is true to an extent. The obliteration o...       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train.shape\n",
    "# df_cv.head()\n",
    "df_cv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "# count_vectorizer.fit(train_df[\"text\"])\n",
    "count_vectorizer.fit(df_train['text'])\n",
    "train_vectors = count_vectorizer.transform(df_train['text'])\n",
    "cv_vectors = count_vectorizer.transform(df_cv['text'])\n",
    "# test_vectors = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = linear_model.RidgeClassifier()\n",
    "\n",
    "# clf = linear_model.\n",
    "clf.fit(train_vectors, df_train[\"target\"])\n",
    "cv_target = clf.predict(cv_vectors)\n",
    "train_target = clf.predict(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_predicted = pd.DataFrame({'predicted': cv_target})\n",
    "df_cv = pd.concat((df_cv, df_cv_predicted), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target       668\n",
       "predicted    855\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv[df_cv['predicted'] == 1][['target', 'predicted']].sum()  # precision is 77%\n",
    "\n",
    "# this looks good. Buy why is ridge \n",
    "# df_cv[(df_cv['predicted'] == 1) & (df_cv['target'] == 1)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.998254037538193\n",
      "recall: 0.9934839270199827\n",
      "f1: 0.9958632701937732\n"
     ]
    }
   ],
   "source": [
    "# train is much better\n",
    "print('precision: ' +  str(precision_score(y_pred=train_target, y_true=df_train['target'])))\n",
    "print('recall: ' + str(recall_score(y_pred=train_target, y_true=df_train['target'])))\n",
    "print('f1: ' +  str(f1_score(y_pred=train_target, y_true=df_train['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.7812865497076024\n",
      "recall: 0.6893704850361198\n",
      "f1: 0.7324561403508772\n",
      "auc: 0.7735825809211018\n"
     ]
    }
   ],
   "source": [
    "print('precision: ' +  str(precision_score(y_pred=cv_target, y_true=df_cv['target'])))\n",
    "print('recall: ' + str(recall_score(y_pred=cv_target, y_true=df_cv['target'])))\n",
    "print('f1: ' +  str(f1_score(y_pred=cv_target, y_true=df_cv['target'])))\n",
    "print('auc: ' +  str(roc_auc_score(y_score=cv_target, y_true=df_cv['target'])))\n",
    "\n",
    "\n",
    "# print('recall': str(recall_score(y_pred=cv_target, y_true=df_cv['target'])))\n",
    "\n",
    "# df_result.loc[i, 'train_recall'] = recall_score(y_pred=df_train['predicted_class'], y_true=df_train[y_var])\n",
    "# df_result.loc[i, 'train_accuracy'] = accuracy_score(y_pred=df_train['predicted_class'], y_true=df_train[y_var])\n",
    "# df_result.loc[i, 'train_auc'] = roc_auc_score(y_true=df_train[y_var], y_score=df_train['predicted_proba'])\n",
    "# df_result.loc[i, 'train_f1_score'] = f1_score(y_pred=df_train['predicted_class'], y_true=df_train[y_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try RNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the viewer, you can submit the above file to the competition! Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
